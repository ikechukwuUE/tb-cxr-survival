{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cc95fff",
   "metadata": {},
   "source": [
    "# ðŸ« Multimodal Survival Modeling in Pulmonary Tuberculosis\n",
    "#### Chest X-ray + Clinical Covariates via Cross-Modal Attention\n",
    "\n",
    "**Author:** Dr. Ikechukwu Ephraim Ugbo, MD\n",
    "**Project:** Innovative AI Healthcare Solutions\n",
    "**Framework:** TensorFlow / Keras\n",
    "\n",
    "#### ---\n",
    "#### ðŸ”¬ Study Context & SOTA References\n",
    "##### This notebook implements a state-of-the-art (SOTA) multimodal architecture for predicting **time-to-major-complication** in TB patients.\n",
    "\n",
    "##### Moving beyond simple feature concatenation, we implement **Cross-Modal Attention** to allow clinical covariates (e.g., HIV status, Age) to dynamically \"attend\" to specific regions of the chest X-ray. This approach is supported by recent literature:\n",
    "\n",
    "##### **Architecture:** We utilize a transformer-based fusion strategy, which has shown superior performance in handling heterogeneous medical data compared to unimodal baselines.\n",
    "##### **Modality Interaction:** Modeling the complex non-linear interactions between imaging and clinical variables (e.g., \"how does diabetes status alter the risk implication of a cavitary lesion?\") is critical for robust prognosis.\n",
    "##### **Backbone:** We use a DenseNet121 backbone for visual feature extraction, a proven strategy for medical imaging tasks.\n",
    "#\n",
    "#### ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bab2777",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Imports & Setup\n",
    "# ==========================================\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import custom modules\n",
    "# Ensure your src/ folder is accessible in the python path\n",
    "from src.config import *\n",
    "from src.data_utils import (\n",
    "    generate_synthetic_tb_clinical_data, \n",
    "    load_tb_cxr_dataset, \n",
    "    train_val_split,\n",
    "    # Assuming you added this function based on our previous step:\n",
    "    # create_matched_dataframe \n",
    ")\n",
    "from src.model_utils import TBSurvivalNet\n",
    "from src.survival_utils import harrell_c_index\n",
    "from src.training_utils import compile_survival_model\n",
    "from src.explainability_utils import generate_gradcam\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "tf.random.set_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"GPUs Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e9bac",
   "metadata": {},
   "source": [
    "#### 1. Data Preparation & Synchronization\n",
    "\n",
    "**Challenge:** A common issue in multimodal medical AI is data heterogeneity and missing modalities.\n",
    "\n",
    "**Strategy:**\n",
    "1. We load the raw **Shenzhen TB Chest X-ray** dataset.\n",
    "2. We generate synchronized synthetic clinical data (Age, HIV, etc.) that matches the *exact* number of images.\n",
    "3. We create a \"Master Key\" dataframe to ensure Image $i$ always corresponds to Clinical Vector $i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3694bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Data Loading\n",
    "# ==========================================\n",
    "# Define path to your image directory\n",
    "IMAGE_DATA_DIR = \"data/raw/images/\"  # Update this path if different\n",
    "\n",
    "# 1. Generate/Load Clinical Data matched to Images\n",
    "# (Logic: Find all images, generate N clinical rows, save 'master_dataset.csv')\n",
    "if not os.path.exists(\"data/processed/master_dataset.csv\"):\n",
    "    print(\"Generating matched clinical data...\")\n",
    "    # NOTE: Ensure you updated src/data_utils.py with 'create_matched_dataframe'\n",
    "    # If not, use the standalone logic we discussed.\n",
    "    from src.data_utils import create_matched_dataframe \n",
    "    df = create_matched_dataframe(IMAGE_DATA_DIR, output_path=\"data/processed/master_dataset.csv\")\n",
    "else:\n",
    "    print(\"Loading existing master dataset...\")\n",
    "    df = pd.read_csv(\"data/processed/master_dataset.csv\")\n",
    "\n",
    "# 2. Load Images using the fixed loader\n",
    "# This fixes the NameError from the previous version\n",
    "ds_images = load_tb_cxr_dataset(\n",
    "    data_dir=IMAGE_DATA_DIR,\n",
    "    image_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False # Important: Keep order to match tabular data first\n",
    ")\n",
    "\n",
    "# 3. Prepare Tabular Data\n",
    "clinical_features = [\"age\", \"sex\", \"hiv\", \"diabetes\", \"bmi\", \"hemoglobin\", \"albumin\"]\n",
    "X_tabular = df[clinical_features].values.astype(\"float32\")\n",
    "time_event = df[[\"time\", \"event\"]].values\n",
    "\n",
    "print(f\"Clinical Data Shape: {X_tabular.shape}\")\n",
    "print(f\"Total Images Found: {len(df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8ff8ea4",
   "metadata": {},
   "source": [
    "#### 2. Baseline Model: Cox Proportional Hazards\n",
    "\n",
    "Before training the Deep Learning model, we establish a clinical baseline.\n",
    "This helps us quantify the \"added value\" of the imaging modality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934cd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Baseline Cox Proportional Hazards Model\n",
    "# ==========================================\n",
    "from lifelines import CoxPHFitter\n",
    "\n",
    "# Fit Cox Model on Clinical Data Only\n",
    "cph = CoxPHFitter()\n",
    "cph_df = df[clinical_features + [\"time\", \"event\"]]\n",
    "cph.fit(cph_df, duration_col=\"time\", event_col=\"event\")\n",
    "\n",
    "# Evaluate\n",
    "c_index_cox = cph.concordance_index_\n",
    "print(f\"Baseline Clinical-Only C-Index: {c_index_cox:.4f}\")\n",
    "\n",
    "# cph.print_summary() # Uncomment to see Hazard Ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c98446",
   "metadata": {},
   "source": [
    "#### 3. SOTA Multimodal Architecture\n",
    "\n",
    "We implement the **Cross-Modal Attention Network** (TBSurvivalNet).\n",
    "\n",
    "#### How it works:\n",
    "1. **Image Encoder:** `DenseNet121` (without pooling) extracts a $7 \\times 7$ grid of visual features.\n",
    "2. **Tabular Encoder:** Clinical features are projected into a high-dimensional embedding space.\n",
    "3. **Cross-Attention:** The clinical embedding acts as a \"Query\" to attend to the spatial visual \"Keys/Values\". This allows the model to focus on specific lung regions relevant to the patient's specific comorbidities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404ff53e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Model Initialization\n",
    "# ==========================================\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "\n",
    "# 1. Define Visual Backbone\n",
    "# CRITICAL CHANGE: pooling=None.\n",
    "# We need the spatial (7,7,1024) features for Attention, not the global average.\n",
    "image_encoder = DenseNet121(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    input_shape=(224, 224, 3),\n",
    "    pooling=None \n",
    ")\n",
    "image_encoder.trainable = False # Freeze initially\n",
    "\n",
    "# 2. Initialize SOTA Model\n",
    "# (Ensure src/model_utils.py has been updated with the CrossModalAttention class)\n",
    "model = TBSurvivalNet(\n",
    "    image_encoder=image_encoder,\n",
    "    tabular_dim=X_tabular.shape[1],\n",
    "    embed_dim=256\n",
    ")\n",
    "\n",
    "# 3. Build & Summary\n",
    "# Input 1: Images (Batch, 224, 224, 3)\n",
    "# Input 2: Tabular (Batch, N_features)\n",
    "model.build([\n",
    "    (None, 224, 224, 3), \n",
    "    (None, X_tabular.shape[1])\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04adf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Training Setup\n",
    "# ==========================================\n",
    "# Split Data (Train/Val)\n",
    "# Note: In a real run, you must ensure X_img corresponds to these indices.\n",
    "# Here we demonstrate the compilation.\n",
    "\n",
    "model = compile_survival_model(\n",
    "    model,\n",
    "    lr=1e-4 # Lower LR is often better for fine-tuning multimodal models\n",
    ")\n",
    "\n",
    "print(\"Model compiled with Cox Partial Likelihood Loss.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae69bac3",
   "metadata": {},
   "source": [
    "#### 4. Training & Explainability\n",
    "\n",
    "Post-training, we will use **Grad-CAM** to visualize which lung regions the model focused on.\n",
    "\n",
    "> \"Clinicians require transparent, actionable models for adoption in decision-making.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f2b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate_gradcam(\n",
    "#     model,\n",
    "#     image=X_img_val[0],\n",
    "#     tabular_dim=X_tabular.shape[1],\n",
    "#     layer_name=\"conv5_block16_concat\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7efddaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# Placeholder Training Loop\n",
    "# ==========================================\n",
    "history = model.fit(\n",
    "   x=[X_img_train, X_tab_train],\n",
    "   y=y_train_surv, # (time, event)\n",
    "   validation_data=([X_img_val, X_tab_val], y_val_surv),\n",
    "   epochs=20, # Reduced for demo\n",
    "   batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# Save weights for future inference\n",
    "# model.save_weights(\"outputs/models/sota_tb_survival_v1.h5\")\n",
    "print(\"Ready for training loop execution.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e0a9a3",
   "metadata": {},
   "source": [
    "## Interpretation and Next Steps\n",
    "\n",
    "- Compare clinical vs multimodal performance\n",
    "- Perform subgroup analyses (HIV, age)\n",
    "- External validation\n",
    "- Competing risks modeling\n",
    "- Manuscript preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8217a58e",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "This notebook provides a reproducible, interpretable framework\n",
    "for survival modeling in pulmonary tuberculosis using\n",
    "chest X-ray imaging and clinical data.\n",
    "\n",
    "It serves as the foundation for further validation and publication\n",
    "within the **Innovative AI Healthcare Solutions** initiative.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
